{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "- sigmoid関数とtanh関数の逆伝搬の式\n",
    "    - 微分を求めるには，計算グラフによる方法と解析的な方法がある\n",
    "- WordNetを動かす\n",
    "    - PythonからWordNetを動かすには，nltk(Natural Language Toolkit)パッケージをインストールして使う\n",
    "    - 単語の意味ネットワークが構築されており，様々な単語間類似度の手法が提供されている\n",
    "- GRU(Gated Recurrent Unit)\n",
    "    - LSTMよりもパラメータ数が少なく，高速なRNN\n",
    "    - LSTMとの精度差は，タスクやハイパーパラメータの調整によって様々．どちらが良いとは言い切れない．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 付録A sigmoid関数とtanh関数の微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid関数\n",
    "sigmoid関数は次の式で表される\n",
    "$$ y = \\frac{1}{1 + exp(-x)} $$\n",
    "計算グラフは次のような流れになる．  \n",
    "\n",
    "<div style=\"text-align: center\">(x, -1) -> [×] ->(-x) -> [exp] -> (exp(-x), 1) -> [+] -> 1+exp(-x) -> [/] -> 1/(1+exp(-x)) = y -></div>\n",
    "[×]と[+]はすでに扱ったが，[exp]と[/]の微分はまだ説明されていない．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### /ノードの逆伝搬\n",
    "逆数を取るノード\n",
    "$$ y = \\frac{1}{x} $$\n",
    "$x$で偏微分すると，商の微分法から次のようになる\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\frac{0 - 1}{x^2} = -\\Bigl(\\frac{1}{x}\\Bigr)^2 = -y^2 $$\n",
    "よって/ノードでは，上流からの勾配$\\frac{\\partial L}{\\partial y}$に対して，出力した$y$をキャッシュしておいて以下のように返せばよい．  \n",
    "$$ -\\frac{\\partial L}{\\partial y}y^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expノードの逆伝搬\n",
    "自然対数の累乗を取るノード\n",
    "$$ y = e^x $$\n",
    "微分しても$e^x$である．  \n",
    "$$ \\frac{\\partial y}{\\partial x} = e^x = y $$\n",
    "よってexpノードでは，上流からの勾配$\\frac{\\partial L}{\\partial y}$に対して，出力した$y$をキャッシュしておいて以下のように返せばよい．  \n",
    "$$ \\frac{\\partial L}{\\partial y} y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid関数全体の逆伝搬\n",
    "計算グラフをたどり，sigmoid関数は出力$y$をキャッシュしておいて以下のように返せばよい．  \n",
    "$$ \\begin{eqnarray*} \\\\\n",
    "    \\frac{\\partial L}{\\partial y} (-y_{div}^2)(y_{exp})(-1) &=& \\frac{\\partial L}{\\partial y} \\frac{1}{(1 + \\exp(-x))^2} \\exp(-x) \\\\\n",
    "    &=& \\frac{\\partial L}{\\partial y} \\frac{1}{1 + \\exp(-x)} \\frac{exp(-x)}{1 + \\exp(-x)} \\\\\n",
    "    &=& \\frac{\\partial L}{\\partial y} \\frac{1}{1 + \\exp(-x)} \\Big(1 - \\frac{1}{1 + \\exp(-x)} \\Big) \\\\\n",
    "    &=& \\frac{\\partial L}{\\partial y} y(1-y) \\\\\n",
    "\\end{eqnarray*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. tanh関数\n",
    "5章を参照  \n",
    "$$ y = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "の微分を求める．商の微分法を使って\n",
    "$$ \\begin{eqnarray*} \\\\\n",
    "    \\frac{\\partial \\tanh(x)}{\\partial x} &=& \\frac {({e^x + e^{-x}})({e^x + e^{-x}}) - ({e^x - e^{-x}})({e^x - e^{-x}})}{({e^x + e^{-x}})^2} \\\\\n",
    "    &=& 1 - \\Bigl\\{ \\frac {({e^x - e^{-x}})}{({e^x + e^{-x}})} \\Bigr\\}^2 \\\\\n",
    "    &=& 1 - \\tanh(x)^2 \\\\\n",
    "    &=& 1 - y^2\n",
    "\\end{eqnarray*} $$\n",
    "よって, 上流からの勾配$\\frac{\\partial L} {\\partial y}$を使って\n",
    "$$ \\frac{\\partial L} {\\partial x} = \\frac{\\partial L} {\\partial y} \\cdot \\frac{\\partial y} {\\partial x} = \\frac{\\partial L} {\\partial y} (1 - y^2) $$\n",
    "を下流に返せば良い．  \n",
    "ここでyは順伝播の時に保持しておく．RNNレイヤの実装でいうh_next， 最終的な出力を使うことになる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3 まとめ\n",
    "NNの微分は計算グラフで解く方法(A1)と解析的な方法で解く方法(A2)がある．  \n",
    "問題に応じて適宜どちらかを利用する．  \n",
    "複数の方法で問題を解決できることは，時としてとても重要である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 付録B WordNetを動かす\n",
    "WordNetをPythonから利用するには，**NLTK**(Natural Language Toolkit)というライブラリを使う  \n",
    "NLTKには，自然言語処理のための便利な機能が多く用意されている．\n",
    "- 品詞タグ付け\n",
    "- 構文解析\n",
    "- 情報抽出\n",
    "- 意味解析\n",
    "NLTKをインストールするにはpipを使う  \n",
    "<div style=\"text-align: center\">$ pip install nltk</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk# 重い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltkを使うには，データのダウンロードを行う必要がある．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     D:/Programming/machinelearning/nltk...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('wordnet', download_dir=\"D:\\\\Programming\\\\machinelearning\\\\nltk\") # ダウンロード先を適宜変更すること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltkのデータが入っているパスを追加する必要がある．\n",
    "nltk.data.path.append('D:\\\\Programming\\\\machinelearning\\\\nltk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2 Wordnetで同義語を得る\n",
    "carという単語について同義語を取得する  \n",
    "<br>\n",
    "まずはcarという単語にどれだけ異なる意味が存在するのかを確認する．  \n",
    "wordnetでは各単語がsynsetと呼ばれる同義語のグループに分類されている．  \n",
    "同義語を取得するにあたっては，それら複数の意味の中から，どの意味に該当するものかを指定する必要がある．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バージョン変わった？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "carという単語には5つの意味のグループがあることが分かる．  \n",
    "それぞれについている見出し語は，たとえば「car.n.01」なら，carの「noun(名詞)」の1番目の意味のグループであることを意味する．  \n",
    "WordNetのメソッドで引数に単語名を指定する場合，「car」だけでなく「car.n.01」のような見出し語で指定する場合が多い．  \n",
    "<br>\n",
    "carという単語の意味を確認してみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car = wordnet.synset('car.n.01') # 見出し語を指定するときはsynsetsではなくsynset\n",
    "car.definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このcarグループにはどのような同義語が存在するのか確認してみよう．  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3 WordNetと単語ネットワーク\n",
    "他の単語との意味的な上位，下位の関係性について調べる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('object.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('artifact.n.01'),\n",
       " Synset('instrumentality.n.03'),\n",
       " Synset('container.n.01'),\n",
       " Synset('wheeled_vehicle.n.01'),\n",
       " Synset('self-propelled_vehicle.n.01'),\n",
       " Synset('motor_vehicle.n.01'),\n",
       " Synset('car.n.01')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.hypernym_paths()[0] # hypernym: 上位語という意味，\n",
    "\n",
    "# [0]のように，リストが返されているのは，carに到着するまでの経路が複数存在するから．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4 WordNetによる意味の類似度\n",
    "WordNetでは，単語間の意味的なネットワークが構築されており，様々な問題で有用である．  \n",
    "ここでは，単語間の類似度(意味経路の類似度)を求めてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car : novel 0.05555555555555555\n",
      "car : dog 0.07692307692307693\n",
      "car : motorcycle 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "novel = wordnet.synset('novel.n.01')\n",
    "dog = wordnet.synset('dog.n.01')\n",
    "motorcycle = wordnet.synset('motorcycle.n.01')\n",
    "\n",
    "print(\"car : novel\", car.path_similarity(novel))\n",
    "print(\"car : dog\", car.path_similarity(dog))\n",
    "print(\"car : motorcycle\", car.path_similarity(motorcycle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他にも，Leacock-Chodorow類似度やWu-Palmer類似度など，いくつかの類似度計測のための手法が用意されている．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "LSTMはパラメータが多い．  \n",
    "LSTMに代わるゲート付きRNNとして，**GRU**(Gated Recurrent Unit)を紹介  \n",
    "GRUはパラメータがLSTMに比べて少ないため，計算時間が短縮できる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1 GRUのインタフェース\n",
    "LSTMが隠れ状態$h_{t-1}$と記憶セル$c_{t-1}$の2つを受け取るのに対し，GRUは$h_{t-1}$のみを使用する．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2 GRUの計算グラフ\n",
    "GRUの内部で行われる計算は以下\n",
    "$$ \\begin{eqnarray*} \\\\\n",
    "    z &=& \\sigma(x_tW_x^{(z)} + h_{t-1}W_h^{(z)} + b^{(z)}) \\\\\n",
    "    r &=& \\sigma(x_tW_x^{(r)} + h_{t-1}W_h^{(r)} + b^{(r)}) \\\\\n",
    "    \\widetilde{h} &=& \\tanh(x_tW_x + (r \\odot h_{t-1})W_h + b) \\\\\n",
    "    h_t &=& (1-z) \\odot h_{t-1} + z \\odot \\widetilde{h} \\\\\n",
    "\\end{eqnarray*} $$\n",
    "計算グラフは省略  \n",
    "$r$はリセットゲート，$z$はupdateゲートと呼ばれる．  \n",
    "<br>\n",
    "$r$は過去の隠れ状態をどれだけ無視するのかを決定し，新しい隠れ状態$\\widetilde{h}$とする．  \n",
    "$z$は隠れ状態を更新するゲートで，LSTMのforgetゲートとinputゲートの2つの役割を担う．  \n",
    "$(1-z) \\odot h_{t-1}$の部分がforgetゲートの機能，$z \\odot \\widetilde{h}$はinputゲートとして機能を担う．  \n",
    "<br>\n",
    "以上のように，GRUはLSTMをよりシンプルにしたアーキテクチャになる．  \n",
    "実装はcommon/time_layers.pyにある．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMとGRUでは，タスクやハイパーパラメータの調整によって勝者は変動する．  \n",
    "最近ではLSTMやLSTMの亜種の方が多く使われつつあるが，GRUも人気を集めている．  \n",
    "GRUはパラメータ数や計算量が少ないので，データセットのサイズが小さい場合や，モデル設計で繰り返しの施行が必要な場合に適しているといえる．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
