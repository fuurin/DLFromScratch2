{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNやLSTMを使った楽しいアプリケーション\n",
    "- コーパスを使って学習した言語モデルを使った文章生成とその改良  \n",
    "- seq2seq(sequence to sequence)\n",
    "    - 2つのRNNを組み合わせて実装する\n",
    "    - 機械翻訳やチャットボット，メールの自動返信などに使える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 言語モデルを使った文章生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNによる文章生成の手順\n",
    "例えば，\n",
    "<div style=\"text-align: center\">you say goodbye and I say Hello . </div>  \n",
    "\n",
    "というコーパスで学習を行った言語モデルでは，「I」を入力すると次に出現する単語は「say」の確率が高くなる．  \n",
    "このことを利用して文章生成ができる．  \n",
    "<br>\n",
    "もっとも確率の高い単語を選び続ければ結果は一意に定まってしまう．(決定的なアルゴリズム)  \n",
    "しかし，確率的に単語を選べば毎回異なった文章が生成できる．  \n",
    "この作業を望む回数だけ，または<eos\\>のような文末記号が出現するまで繰り返す．  \n",
    "このようにしてできた文章は，訓練データには存在しない文章の並びになりうる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章生成の実装\n",
    "前章で実装したRnnlmクラスをベースに，それを継承してRnnlmGenクラスを生成する.  \n",
    "このクラスに文章生成を行うメソッドを追加する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import softmax\n",
    "from ch06.rnnlm import Rnnlm\n",
    "\n",
    "class RnnlmGen(Rnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
    "        \"\"\"\n",
    "        言語モデルから文章の生成を行う．\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        start_id: int\n",
    "            文章生成の際の最初の単語のID\n",
    "        skip_ids: Array<int>\n",
    "            文章生成の際サンプリングしたくない単語のID\n",
    "            PTBデータセットの<unk>(レアな単語)やN(数値)などの前処理されたデータをサンプリングしないようにする用途で使う\n",
    "        sample_size: int = 100\n",
    "            生成する文章の長さ\n",
    "        \"\"\"\n",
    "        word_ids = [start_id]\n",
    "        \n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1) # 文章の長さ1,バッチサイズも1の入力データに変換\n",
    "            score = self.predict(x) # 次の単語を予測\n",
    "            p = softmax(score.flatten()) # 予測結果の確率分布を取得(softmaxで正規化)\n",
    "            \n",
    "            sampled = np.random.choice(len(p), size=1, p=p) # 予測結果の確率分布を使ってp個の単語の中から次の単語1個をサンプル\n",
    "            \n",
    "            if (skip_ids is None) or (sampled not in skip_ids): # skip_idが空ならSkip_IDの探索を行わない\n",
    "                x = sampled # Skipワードでなければこの単語を次の探索に使う\n",
    "                word_ids.append(int(x))\n",
    "        \n",
    "        return word_ids # 生成された文章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは重みパラメータがランダムな初期値の状態で文章作成を行わせてみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you need to the point including u.s. exports.\n",
      " they were jackets.\n",
      " while the gap carried into a rapidly stadium during the last three years with his policyholders led to mr. roman who has stepped up some if certainly in moscow he says.\n",
      " mr. roman is friendly.\n",
      " the contends he is a popular chairman of making bargain.\n",
      " it was no way to estimates that time trial was able to provide an ronald dramatic surge over the entire wage would generate meals and president bush.\n",
      " we would stay the debt of having gone.\n",
      " new contacts\n"
     ]
    }
   ],
   "source": [
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params('./ch06/Rnnlm.pkl')\n",
    "\n",
    "# start文字とskip文字の設定\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "# 文章生成\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期値で文章を生成した結果の一例  \n",
    "<br>\n",
    "you heating destroying split yale proteins teacher flowing shoulder collection knowledgeable whitten pence cancer balance buildings geared profession growing tonight hud compounded franchisers still cooperative reitman settlement benjamin bail lenses johns sometimes processing quickly arab wins edge men asia louis rate statute short destruction founded contrast potato robust steadily net either shapiro contest begins participate buffer accounted cautiously travel embraced refunding merchandising passenger referring issuer shaking ring vintage largest john passion resign urged them poland sandinistas anderson native dominates tire bruce justified istat des dean westridge perceptions talk enough brisk handed airline foot panic sedan scores transport exodus 24-hour championship  \n",
    "<br>\n",
    "でたらめである．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みのパラメータを使って文章を生成した結果の一例  \n",
    "<br>\n",
    "you get the people.\n",
    "we 'll have offered sound to approach wood.\n",
    "the game 's effort has shown in australia or the additional year because of which the grocery circuit plans to assist in reins the vice securities known as directors of manufacturers hanover corp. 's employee plans to be provided impact on the problem.\n",
    "state the losses are bonuses at the number of expires tomorrow.\n",
    "the largest u.s. operations is increasingly offset by tuesday 's goal but expensive purchases by itself morgan competitive authorities said.\n",
    "roberti sciences studies typically trust to redeem steadily customers  \n",
    "<br>\n",
    "文法的に正しい文章もあるようだが，意味の通じない文章も散見される．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## さらに良い文章へ\n",
    "Better Rnnlmを使ったらもうちょっと良い文章になるのでは？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch06.better_rnnlm import BetterRnnlm\n",
    "\n",
    "class BetterRnnlmGen(BetterRnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
    "        \"\"\"\n",
    "        言語モデルから文章の生成を行う．\n",
    "        \n",
    "        Parameters\n",
    "        --------------\n",
    "        start_id: int\n",
    "            文章生成の際の最初の単語のID\n",
    "        skip_ids: Array<int>\n",
    "            文章生成の際サンプリングしたくない単語のID\n",
    "            PTBデータセットの<unk>(レアな単語)やN(数値)などの前処理されたデータをサンプリングしないようにする用途で使う\n",
    "        sample_size: int = 100\n",
    "            生成する文章の長さ\n",
    "        \"\"\"\n",
    "        word_ids = [start_id]\n",
    "        \n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1) # 文章の長さ1,バッチサイズも1の入力データに変換\n",
    "            score = self.predict(x) # 次の単語を予測\n",
    "            p = softmax(score.flatten()) # 予測結果の確率分布を取得(softmaxで正規化)\n",
    "            \n",
    "            sampled = np.random.choice(len(p), size=1, p=p) # 予測結果の確率分布を使ってp個の単語の中から次の単語1個をサンプル\n",
    "            \n",
    "            if (skip_ids is None) or (sampled not in skip_ids): # skip_idが空ならSkip_IDの探索を行わない\n",
    "                x = sampled # Skipワードでなければこの単語を次の探索に使う\n",
    "                word_ids.append(int(x))\n",
    "        \n",
    "        return word_ids # 生成された文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel.load_params('./ch06/BetterRnnlm.pkl') # 自分で作るしかない…\\n\\n# start文字とskip文字の設定\\nstart_word = 'you'\\nstart_id = word_to_id[start_word]\\nskip_words = ['N', '<unk>', '$']\\nskip_ids = [word_to_id[w] for w in skip_words]\\n\\n# 文章生成\\nword_ids = model.generate(start_id, skip_ids)\\ntxt = ' '.join([id_to_word[i] for i in word_ids])\\ntxt = txt.replace(' <eos>', '.\\n')\\nprint(txt)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BetterRnnlmGen()\n",
    "\"\"\"\n",
    "model.load_params('./ch06/BetterRnnlm.pkl') # 自分で作るしかない…\n",
    "\n",
    "# start文字とskip文字の設定\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "# 文章生成\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print(txt)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " これを実行するとなかなか自然な文が生成される．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上では最初の単語のみを与えたが，「the meaning of life is」のように複数の単語を与えることもできる．  \n",
    "この結果はなんだか深いのが返ってくる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq\n",
    "ある時系列データを別の時系列データに変換する．  \n",
    "例えば機械翻訳，チャットボット，コンパイラなど  \n",
    "ここでは2つのRNNを利用するseq2seq(sequence to sequence)を見ていく．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seqの原理\n",
    "seq2seqはEncoder-Decorderモデルとも呼ばれる．  \n",
    "Encode: 情報をある規則に基づいて符号に変換すること．  A -> 1000001  \n",
    "Decode: Encodeされた情報を元の情報に戻すこと. 1000001 -> A  \n",
    "<br>\n",
    "翻訳を行う例でseq2seqの流れを説明すると以下のようになる．  \n",
    "<div style=\"text-align: center\">「我輩は猫である」 -> Encoder -> コンパクトな情報 -> Decoder -> 「I am a cat」</div>\n",
    "\n",
    "Encoder, DecoderにRNNを使うことができる．  \n",
    "<br>\n",
    "Encoderは各単語ごとにEmbeddingレイヤとLSTMレイヤ(RNNやGRUでもOK)を通し，時系列方向に連結したものである．  \n",
    "最後の時刻のLSTMレイヤが出力するデータを，時系列データ$h$という隠れ状態ベクトルとして出力する．  \n",
    "$h$は固定長のベクトルである．エンコードとは任意の長さのデータを固定長のベクトルに変換することである．  \n",
    "<br>\n",
    "Decoderは前節のRNNLMを使った文章生成モデルとほぼ同様の構成をとる.  \n",
    "違うのは，最初のLSTKレイヤがEncoderから渡されたベクトル$h$を利用し，また最初の単語として<eos\\>などの区切り文字を入力すること．  \n",
    "文章生成モデルではLSTMレイヤは隠れ状態として何も受け取っていなかった．(零ベクトルを受け取っていた)  \n",
    "LSTMにサンプリングの開始/終了を伝えるため，<go\\>, <start\\>, \\_などが使われることもある．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "余談  \n",
    "敵対し合うのがGANなら協力し合うのがseq2seq?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 時系列データ変換用のトイ・プロブレム\n",
    "seq2seqの実装を評価する問題として，足し算を扱う．例えば，  \n",
    "<div style=\"text-align: center\">57+5 -> 62</div>\n",
    "このような，機械学習を評価するために作られた簡単な問題はトイ・プロブレムと呼ばれる．  \n",
    "今回の問題は，文を単語ではなく文字で分割し，\n",
    "<div style=\"text-align: center\">57+5 -> ['5', '7', '+', '5']</div>\n",
    "のようにして扱う．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可変長の時系列データ\n",
    "足し算をCharacterのリストとして扱うと，問題ごとに文字数が異なってしまうことが問題になる．  \n",
    "ミニバッチ学習を行うときには，複数のサンプルをまとめて処理する必要があるので，空白文字でパディングを行う．  \n",
    "また，区切り文字としてアンダースコア「_」を出力の先頭につけ，Decoderに文字列生成を知らせる合図(最初の単語)とする.  \n",
    "<br>\n",
    "最大3桁＋3桁の計算を行う例  \n",
    "<div style=\"text-align: center\">「57+5   」 -> 「_62  」</div>\n",
    "\n",
    "最大で4桁の数(999+999=1998)なので，出力は5文字になる．    \n",
    "<br>\n",
    "パディングの文字まで処理させることになってしまうので，SoftmaxWithLossレイヤにマスク機能を追加し，損失の結果に計上しないようにする．  \n",
    "また，Encoderはパディングが入力された時，前時刻の入力をそのまま出力する．  \n",
    "<br>\n",
    "本章ではわかりやすさを優先して，パディング用の文字に対して特別な処理を行わず，通常のデータとして処理することにする．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 足し算データセット\n",
    "50000個の足し算の例がdataset/addition.txtにあらかじめ用意されている．  \n",
    "この学習データはKerasのseq2seqの実装例を参考に作成されている．  \n",
    "<br>\n",
    "本書ではseq2seq用の学習データ(txt)を扱う専用モジュールが提供されている．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 7) (45000, 5)\n",
      "(5000, 7) (5000, 5)\n",
      "[ 3  0  2  0  0 11  5]\n",
      "[ 6  0 11  7  5]\n",
      "71+118 \n",
      "_189 \n"
     ]
    }
   ],
   "source": [
    "from dataset import sequence\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt', seed=1984) # 固定で10％がテストデータになる.seedはシャッフルの．\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "print(x_train.shape, t_train.shape) # (45000, 7) (45000, 5)\n",
    "print(x_test.shape, t_test.shape) # (5000, 7) (5000, 5)\n",
    "\n",
    "print(x_train[0]) # 最初の足し算の問題(データはIDで入っている)\n",
    "print(t_train[0]) # 最初の足し算の答え(データはIDで入っている)\n",
    "\n",
    "print(''.join([id_to_char[c] for c in x_train[0]]))\n",
    "print(''.join([id_to_char[c] for c in t_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seqの実装\n",
    "EncoderクラスとDecoderクラスをそれぞれ実装する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoderクラス\n",
    "EncoderはLSTMの隠れ状態だけをDecoderに渡す．  \n",
    "記憶セルをDecoderに渡すことは可能だが，LSTM自身だけで利用する前提として設計されているため，一般的には行われていない．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.time_layers import TimeEmbedding, TimeLSTM\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_W = (rn(V, D) /100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        \n",
    "        # 前章までの言語モデルは長い時系列データがひとつ存在するものだった(ブロック同士はつながりをもっていた)\n",
    "        # ここでは短い時系列データが複数存在する問題なので，問題ごとにLSTMの隠れ状態をリセットするようstateful=Falseに設定する．\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "        \n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return xs[:, -1, :] # 最後の時刻のデータ，Decorderに渡す\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:, -1, :] = dh # 勾配の初期値として最後の勾配をDecorderから渡ってきた勾配にする．\n",
    "        \n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoderクラス\n",
    "同じ順方向でも，学習のforwardと文章生成のgenerateでは挙動が違うことに注意する．  \n",
    "入力データは['\\_', '6', '2', ' ']を与え，対応する出力として['6', '2', ' ', ' ']を教師データとして与える．  \n",
    "<br>\n",
    "また，文章生成の時は確率的に出力単語を選んだが，ここではもっとも高いスコアを持つ文字を決定的に選んでいく．(計算なので）   \n",
    "最大スコアを持つインデックスを選ぶノードとしてargmaxノードを登場させる．  最大のものを選べば良いので，Softmaxをする必要はない．  \n",
    "損失を計算して逆伝播に流すSoftmaxWithLossはseq2seqクラスで使う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.time_layers import TimeAffine\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        \n",
    "        # 頭のhをset -> forwardという流れで学習を行う．\n",
    "        # このとき，statefulがFalseだと，forwardするときにせっかくEncoderから渡されたhをリセットしてしまうので，Trueにしておく．\n",
    "        # 前章では，前時刻のLSTMブロックから次のLSTMブロックへのつながりを表現するためにstatefulをTrueにセットしていた．\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "    \n",
    "    def forward(self, xs, h):\n",
    "        self.lstm.set_state(h) # EncodeされたsequenceをLSTMに入力\n",
    "        \n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out)\n",
    "        score = self.affine.forward(out)\n",
    "        return score\n",
    "    \n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        dh = self.lstm.dh # TimeLSTMのhの勾配をEncoderに流す\n",
    "        return dh\n",
    "    \n",
    "    # 同じ順方向でも，文章生成のforwardとは違うデータの与え方をする．\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape((1, 1))\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            score = self.affine.forward(out)\n",
    "            \n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "            \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seqクラス\n",
    "EncoderクラスとDecoderクラスをつなぎ合わせ，TimeSoftmaxWithLossレイヤを使って損失を計算し，逆伝播をおこなう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.base_model import BaseModel\n",
    "from common.time_layers import TimeSoftmaxWithLoss\n",
    "\n",
    "class Seq2seq(BaseModel):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "        \n",
    "    def forward(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "        \n",
    "        h = self.encoder.forward(xs)\n",
    "        score = self.decoder.forward(decoder_xs, h)\n",
    "        loss = self.softmax.forward(score, decoder_ts)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.softmax.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "        return dout\n",
    "    \n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        h = self.encoder.forward(xs)\n",
    "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seqの評価\n",
    "足し算問題を解いてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (128,16) and (128,512) not aligned: 16 (dim 1) != 128 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f62dfa77fddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0macc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcorrect_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programing/machinelearning/DLFromScratch2/common/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# 勾配を求め、パラメータを更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 共有された重みを1つに集約\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-226de69d3790>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs, ts)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programing/machinelearning/DLFromScratch2/ch07/seq2seq.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs, h)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programing/machinelearning/DLFromScratch2/common/time_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programing/machinelearning/DLFromScratch2/common/time_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h_prev, c_prev)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (128,16) and (128,512) not aligned: 16 (dim 1) != 128 (dim 0)"
     ]
    }
   ],
   "source": [
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq # 足し算の正解数を数える関数\n",
    "\n",
    "# データセットの読み込み\n",
    "(x_train, t_train), (x_text, t_test) = sequence.load_data('addition.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "max_epoch = 25\n",
    "max_grad = 5.0\n",
    "\n",
    "# モデル / オプティマイザ / トレーナーの作成\n",
    "model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,batch_size=batch_size, max_grad=max_grad)\n",
    "    \n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10 # テストデータの最初の10個を結果表示\n",
    "        correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose) # 入力文を反転したかを指定するis_reverseもある\n",
    "        \n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
