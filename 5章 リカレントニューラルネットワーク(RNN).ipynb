{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで我々が見てきたNNは，フィードフォワードと呼ばれるネットワークで，流れが1方向でしかなかった.  \n",
    "フィードフォワードでは時系列データをうまく扱うことができない．  \n",
    "そこでリカレントニューラルネットワーク(RNN)の出番である．  \n",
    "本章では，フィードフォワードの問題点を指摘し，RNNがその問題を解決することを示す．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まとめ\n",
    "- 言語モデル\n",
    "    - 単語の羅列の自然さなどを確率として解釈するモデル．\n",
    "    - RNNレイヤを利用した条件付き言語モデルは，それまで登場した単語の情報を記憶することができる．\n",
    "    - 言語モデルの評価は，パープレキシティという正解の確率の逆数を用いた指標を使う．\n",
    "- RNNレイヤ\n",
    "    - ループする経路を持ち，隠れ状態として，その時刻で出力した単語(の分散表現)を内部に記憶し，学習に利用するレイヤ\n",
    "        - 今自分なんて言った？を覚えておく\n",
    "    - RNNのループ経路を展開することで，複数のRNNレイヤが繋がったネットワークを形作る．\n",
    "    - これにより，長大なコンテキストを時系列データとして学習できる．  \n",
    "        - これらの単語の次はこの単語がくる，という情報を学習していく\n",
    "    - 展開されたRNNの重み更新は誤差逆伝播法で行うことができ，これをBack Propergation Through Time (BPTT)と呼ぶ．\n",
    "- Time RNNレイヤ\n",
    "    - 長い時系列データを学習する場合は，適当な長さでデータのまとまりを作り，それらのブロック単位で誤差逆伝播法を行う．これをTruncated BPTTと呼ぶ．\n",
    "    - Truncated BPTTでは順伝播の時，データの順番は保ったまま，開始点をずらしてデータを学習させる．  \n",
    "- RNNLM\n",
    "    - RNNによる言語モデル，すなわち文章の自然性学習器を実装\n",
    "    - これでは非常に大きなコーパスを扱うことができない．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確率と言語モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vecを確率の視点から眺める\n",
    "\n",
    "コーパス$w_1, w_2, ... w_T$が与えられているとき，$w_t$がターゲットとなる確率は，コンテキスト$w_{t-1}, w{t+1}$を使って\n",
    "$$ P(w_t | w_{t-1}, w_{t+1})$$\n",
    "と書ける．  \n",
    "ここで，コンテキストの窓を非対称にして全て左側にコンテキストがあるとすると，$w_t$がターゲットとなる確率は\n",
    "$$ P(w_t | w_{t-2}, w_{t-1})$$\n",
    "このとき，CBOWモデルが扱う損失関数は\n",
    "$$ L=-\\log P(w_t|w_{t-2}, w_{t-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語モデル\n",
    "言語モデルは，単語の並びがどれだけ自然であるかを確率で評価する．  \n",
    "例えば，\n",
    "- you say goodbye -> 0.092\n",
    "- you say good die -> 0.000000000000032  \n",
    "\n",
    "$ w_1, ..., w_m $ という順序で単語が出現する確率は，同時確率 $ P(w_1, ..., w_m) $ で表される．  \n",
    "これを事後確率と確率の乗法定理 $P(A, B) = P(A|B)P(B)$ を使って分解すると　　\n",
    "$$ P(w_1, ..., w_m) = P(w_m|w_1, ... w_{m-1})P(w_{m-1}|w_1, ... w_{m-2}) ... P(w_3|w_1, w_2)P(w_2|w_1)P(w_1)$$  \n",
    "$$ = \\prod_{t=1}^{m} P(w_t|w_1, ..., w_{t-1})$$\n",
    "\n",
    "確率の乗法定理は，「AとBが両方同時に起こる確率 $P(A,B)$」は，「Bが起こる確率$P(B)$」と「Bが起こったあとにAが起きる確率P(A|B)」を掛け合わせたものである． \n",
    "また，$P(A, B) = P(B|A)P(A)$と書くこともできる．  \n",
    "ここで注目すべきは，事後確率が対象の単語より左の全ての単語をコンテキストとした時の確率ということである．  \n",
    "また， $P(w_t|w_1, ... w_{t-1})$を表すモデルは，条件付き言語モデルと呼ばれる．これを言語モデルと呼ぶ場合も多く見られる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOWモデルを言語モデルに？\n",
    "CBOWモデルを無理やり言語モデルに適用するには，コンテキストのサイズをある値に限定することで近似的に表すことができる．  \n",
    "$$ P(w_1, ..., w_m) = \\prod_{t=1}^{m} P(w_t|w_1, ..., w_{t-1}) \\approx \\prod_{t=1}^{m} P(w_t|w_{t-2}, w_{t-1}) $$\n",
    "  \n",
    "マルコフ性  \n",
    "未来の状態が現在の状態だけに依存して決まること．  \n",
    "ここで，ある事象の確率がその直前のN個の事象だけに依存するとき，これを「N階マルコフ連鎖」という． 　\n",
    "今は直前の2つに依存して次の単語が決まるので，2階マルコフ連鎖と呼べる．  \n",
    "  \n",
    "コンテキストのサイズは任意に設定できるが，固定する必要があるところに問題がある．  \n",
    "例えば，コンテキストのサイズが10だが答えとなるTomなどの固有名詞が18個前にしかない時，この推論問題に答えることはできない．  \n",
    "コンテキストを20にしたりすれば答えることはできる．  \n",
    "  \n",
    "しかし次にはコンテキスト内の単語の並びが無視されるという問題がある．  \n",
    "例えば，(you,say)というコンテキストと(say,you)というコンテキストが同じものとして表される．  \n",
    "これは，CBOWモデルの中間層を各コンテキストが共有しているために起きる．  \n",
    "そこで，コンテキストごとに中間層を設けることで，すなわち複数の中間層を「連結(contcatenate)」することでこの問題に対処できる．  \n",
    "しかし，そのようにするとコンテキストのサイズに比例して重みパラメータが増大してしまう．  \n",
    "\n",
    "これらの問題を解決するのがRNNである．  \n",
    "RNNは，コンテキストがどれだけ長くても，そのコンテキストの情報を記憶するメカニズムを持つ．  \n",
    "  \n",
    "ちなみに，実はword2vecの方が後に提案されている．  \n",
    "RNNによる言語モデルでも単語の分散表現を獲得できるのだが，新たな語彙の追加しやすさや単語の分散表現の質の向上のためword2vecが提案された．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNとは\n",
    "Recurrent Neural Networkは日本語では「再起ニューラルネットワーク」や「循環ニューラルネットワーク」と呼ばれる．  \n",
    "これに対してRecursive Neural Networkというものもあるが，こちらは主に木構造のデータを処理するためのネットワークで，RNNとは別物である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循環するニューラルネットワーク\n",
    "RNNの特徴は，閉路を持つことである．  \n",
    "入力データを$(x_0, x_1, ... x_t, ...)$として， 出力データ$(h_0, h_1, ... h_t, ...)$ を自分にも入力する，すなわち閉路を持つ層をRNNレイヤと名付ける．　　\n",
    "ここで，　$x_t$や$h_t$はベクトルを想定する．例えば，ある単語の分散表現を$x_t$としたりする．  \n",
    "また，これまではデータが左から右へ流れていたが，以降は左右方向にレイヤが展開されるため，紙面の都合上，下から上へデータが流れるように描画される．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ループの展開\n",
    "RNNレイヤは自身に出力データを流していたが，これを同じレイヤの別のRNNレイヤに流すことで，ループを展開する．  \n",
    "左から右へ，同じレイヤのRNNレイヤが並び，その順が時系列の順番になっている．  \n",
    "その出力が，左から$h_0, h_1, ... h_t$となる．  \n",
    "「t番目の単語」や「t番目のRNNレイヤ」は，「時刻tの単語」や「時刻tのRNNレイヤ」というように，「時刻」という言葉でも表される．  \n",
    "このとき，$h_t$は以下の計算で算出される．  \n",
    "$$ h_t = \\tanh (h_{t-1} W_h + x_t W_x + b) $$\n",
    "RNNレイヤは重みを2つ持つ．  \n",
    "入力$x$への重み$W_x$と一つ前のRNNの出力$h_{t-1}$に対する重み$W_h$である．  \n",
    "これらの和にさらにバイアス$b$を加え， 活性化関数としてtanhを適用したものがRNNレイヤの出力になっている．  \n",
    "  \n",
    "この式では，RNNが$h_t$という現在の出力を状態として保持していて，上記の式で$h_t$が更新されていると見ることもできる．  \n",
    "そのため，RNNレイヤは状態を持つレイヤやメモリを持つレイヤと呼ばれている．  \n",
    "  \n",
    "この$h_t$は「隠れ状態」や「隠れ状態ベクトル」と呼ばれている．  \n",
    "  \n",
    "多くの文献では，RNNレイヤの次の時刻への矢印はRNNレイヤから生えているが，本書では同じデータがコピーして分岐されたものであることを明示的に示すため，RNNレイヤの次の層への矢印から次の自国への矢印を伸ばすことにする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time\n",
    "ループを展開した後のRNNは，時間方向と逆方向に誤差逆伝播法を使って重みを更新していくことができる．  \n",
    "このようなRNNにおける誤差逆伝播ん法をBackpropagation Through Time, 略してBPTTと呼ぶ.  \n",
    "  \n",
    "しかし，RNNレイヤを長く繋げると計算リソース(RNNが使うメモリ)が膨大になり，逆伝播の勾配も不安定になってしまう．　　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated BPTT\n",
    "時間方向に長くなりすぎたネットワークを適当な場所で切り取って(truncate)小さなネットワークを複数作ること．  \n",
    "これら小さくなったネットワークに対して，誤差逆伝播法を行う．  \n",
    "正確には，ネットワークの逆伝播の時だけつながりを断ち切る．順伝播はそのまま行う．  \n",
    "  \n",
    "具体例を見ていく.  \n",
    "1000個の長さのコーパス$x_0, x_1, ... x_{999}$があるとする． このコーパスは，複数の文が連結されたものであるとする．  \n",
    "Truncated BPTTをするため，逆伝播のときは1000個連結されたRNNを「ブロック」に切り分ける．  \n",
    "ここでは，それぞれのブロックのRNNのレイヤの長さが10になるようにする．  これによって，それぞれのブロックではそれより未来のデータについて考えなくていい．  \n",
    "\n",
    "順伝播の時は，データを順番に(シーケンシャルに)与える必要がある．  \n",
    "これまでミニバッチ学習ではデータをランダムに選んでいたが，Truncated BPTTでは順伝播の時にデータをシーケンシャルに与える必要がある．  \n",
    "  \n",
    "上の例でいうと，まずは一つ目のブロックの入力データ$x_0, ... x_9$をRNNレイヤに与え, forwardとbackwardを行う．  \n",
    "次は，入力データ$x_{10}, ... ,x_{19}$をRNNに与え, forwardとbackwardを行う．  \n",
    "このとき，前のブロックの出力$h_9$をこのブロックの先頭のRNNに与える必要があることに注意する．これによって順伝播のつながりは維持できる.  \n",
    "以上のようにして順伝播とTruncated BPTTを進めていく．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated BPTTのミニバッチ学習\n",
    "RNNでTruncated BPTTする場合のミニバッチ学習では，開始位置をずらしたデータを利用する．  \n",
    "これによって，各単語の前後関係を保ったまま，つまりシーケンシャルなまま，違うデータをRNNに与えることができる．  \n",
    "\n",
    "例えば上の例では$x_0$からデータを与えていったが，別のデータでは$x_{500}$からデータを与えていく．  \n",
    "なお，途中で終端に達した場合は，先頭に戻る．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNの実装\n",
    "横方向にRNNをT個だけ繋げたものを一つの層として実装する．  \n",
    "この層は，シーケンシャルなデータ$xs = (x_0, x_1, ... x_{T-1})$を入力すると，隠れ層へ$hs=(h_0, h_1, ..., h_{T-1})$を出力する．  \n",
    "このひとまとまりにした層を「Time RNN」レイヤと呼ぶ．↔️ 1ステップの処理を行う「RNNレイヤ」  \n",
    "後ほどTime Affine レイヤやTime Embeddingレイヤといった，時系列データをまとめて処理するレイヤも登場する．  \n",
    "\n",
    "まず1ステップの処理を行うRNNクラスを作成し，Tステップの処理をまとめて行うレイヤをTimeRNNクラスとして完成させる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNレイヤの実装\n",
    "RNNレイヤの計算式を再掲\n",
    "$$ h_t = \\tanh (h_{t-1} W_h + x_t W_x + b) $$\n",
    "$x_t$と$h_{t-1}$はミニバッチ処理のため，$N$個のデータを持つとする．各行がデータで，それらが列方向に並ぶ．  \n",
    "そのため，$x_t$と$h_{t-1}$はそれぞれ$N \\times H $, $N \\times D$の行列になる．  \n",
    "ここで，$H$は隠れ状態ベクトルの次元数， $D$は入力データの次元数である．  \n",
    "RNNレイヤの計算では，次元数が以下のようになることに注意するのが重要である．\n",
    "\n",
    "$$ h_{t-1} W_h + x_t W_x + b \\Rightarrow h_t$$\n",
    "$$ (N \\times H) (H\\times H) + (N \\times D) (D \\times H) + (N \\times H) \\Rightarrow (N \\times H) $$  \n",
    "\n",
    "以下この式とシステムに従いRNNレイヤを実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b] # 初期重みをparamsに設定\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)] # 初期勾配をgradsに設定\n",
    "        self.cache = None # BPTT用のキャッシュ変数(メモリ)\n",
    "    \n",
    "    def forward(self, x, h_prev): # 入力データと前の時刻の出力\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "        \n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "    \n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        # tanhの逆伝播\n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        \n",
    "        # 加算ノードはdtをそのまま前のレイヤに返す\n",
    "        \n",
    "        # b方向へのrepeatの逆伝播\n",
    "        db = np.sum(dt, axis=0)\n",
    "        \n",
    "        # h_prev方向へのMatMulの逆伝播\n",
    "        dWh = h_prev.T @ dt\n",
    "        dh_prev = dt @ Wh.T\n",
    "        \n",
    "        # x方向へのMatMulの逆伝播\n",
    "        dWx = x.T @ dt\n",
    "        dx = dt @ Wx.T\n",
    "        \n",
    "        self.grads[0][...] = dWx # 3点リーダによって，浅いコピー(参照のコピー)ではなく深いコピー(値のコピー)を行う．\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでちょっと逆伝播の復習\n",
    "\n",
    "- 加算ノードの逆伝播  \n",
    "上流から入ってきた勾配$\\frac{\\partial L}{\\partial z}$をそのまま受け流す. 3個以上の加算ノードでも同様にそのまま受け流す．\n",
    "$$ z = x + y $$\n",
    "$$ \\frac{\\partial z}{\\partial x} = 1, \\frac{\\partial z}{\\partial y} = 1 $$\n",
    "上流で発生した損失Lに対するxの勾配\n",
    "$$　\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial x} = \\frac{\\partial L}{\\partial z} \\cdot 1 = \\frac{\\partial L}{\\partial z}$$\n",
    "上流で発生した損失Lに対するyの勾配\n",
    "$$ \\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial y} = \\frac{\\partial L}{\\partial z} \\cdot 1 = \\frac{\\partial L}{\\partial z}$$  \n",
    "\n",
    "<br><br>\n",
    "- Repeatノードの逆伝播\n",
    "RNNレイヤの実装では，N個のミニバッチにそれぞれ同じ重さベクトルbを加算している．  \n",
    "これは分岐ノードの一般形であるRepeatノードの処理が行われていることになる．  \n",
    "分岐ノード，Repeatノードでは，上流からやってきた重みを全て足し合わせて下流に返す．  \n",
    "Repeatノードの逆伝播は，$D\\times N$の上流からの勾配をN個の方向に足し合わせ，D次元のベクトルを作り出し，そのベクトルを勾配とする．\n",
    "\n",
    "<br><br>\n",
    "- 行列の積(Matrix Multiply: MatMul)の逆伝播  \n",
    "p30, 31ではシグマを使ってもうちょい厳密に書いているが，自分の理解を書いてみる．まずは結果から\n",
    "$$ y = xW $$\n",
    "$$ (N \\times H) = (N \\times D)(D \\times H) $$\n",
    "$$ \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y}W^{\\top}, \\frac{\\partial L}{\\partial W} = x^{\\top} \\frac{\\partial L}{\\partial y} $$\n",
    "$$ (N \\times D) = (N \\times H)(H \\times D), (D \\times H) = (D \\times N)(N \\times H) $$\n",
    "<br>\n",
    "\n",
    "例えば，以下のような場合を考える．\n",
    "$$ y = xW = \\left( \\begin {array}{ccc} a & b & c \\end{array} \\right) \\left( \\begin {array}{ccc} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{array} \\right) = \\left( \\begin {array}{ccc} 1a+2b+3c & 4a+5b+6c & 7a+8b+9c \\end {array} \\right) $$\n",
    "\n",
    "このとき， [ベクトルをベクトルで微分する計算の定義](https://qiita.com/AnchorBlues/items/8fe2483a3a72676eb96d)を使って\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\left( \\begin{array}{ccc} \\frac{\\partial y_1}{\\partial a} & \\frac{\\partial y_2}{\\partial a} & \\frac{\\partial y_3}{\\partial a}\\\\ \\frac{\\partial y_1}{\\partial b} & \\frac{\\partial y_2}{\\partial b} & \\frac{\\partial y_3}{\\partial b} \\\\ \\frac{\\partial y_3}{\\partial c} & \\frac{\\partial y_2}{\\partial c} & \\frac{\\partial y_3}{\\partial c}\\end{array} \\right) =\\left( \\begin{array}{ccc} 1 & 4 & 7 \\\\ 2 & 5 & 8 \\\\ 3 & 6 & 9 \\end{array} \\right) = W^\\top$$\n",
    "つまり，\n",
    "$$ \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} = \\frac{\\partial L}{\\partial y} W^\\top $$ \n",
    "$\\frac{\\partial L}{\\partial W}　= x^\\top \\frac{\\partial L}{\\partial y}$も同様に求められる．(ベクトル$y$を行列$W$で微分)\n",
    "<br><br>\n",
    "- tanhの逆伝播\n",
    "$$ y = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "の微分を求める．商の微分法を使って\n",
    "$$ \\frac{\\partial \\tanh(x)}{\\partial x} = \\frac {({e^x + e^{-x}})({e^x + e^{-x}}) - ({e^x - e^{-x}})({e^x - e^{-x}})}{({e^x + e^{-x}})^2} $$\n",
    "$$ = 1 - \\Bigl\\{ \\frac {({e^x - e^{-x}})}{({e^x + e^{-x}})} \\Bigr\\}^2 $$\n",
    "$$ = 1 - \\tanh(x)^2 = 1 - y^2 $$\n",
    "よって, 上流からの勾配$\\frac{\\partial L} {\\partial y}$を使って\n",
    "$$ \\frac{\\partial L} {\\partial x} = \\frac{\\partial L} {\\partial y} \\cdot \\frac{\\partial y} {\\partial x} = \\frac{\\partial L} {\\partial y} (1 - y^2) $$\n",
    "を下流に返せば良い．  \n",
    "ここでyは順伝播の時に保持しておく．RNNレイヤの実装でいうh_next， 最終的な出力を使うことになる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time RNN レイヤの実装\n",
    "T個のRNNレイヤで構成されるTime RNNレイヤを実装する．  \n",
    "Time RNNレイヤは隠れ状態hをメンバ変数として保持し，ブロックに分割した時に次のブロックへhを渡す．  \n",
    "隠れ状態を引き継ぐかどうかはstatefulという引数で調整する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False): # stateful: 状態を持つ，という意味\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None # 複数のRNNレイヤをリストとして持つ予定\n",
    "        self.h, self.dh = None, None # forwardの最後の出力(次のブロックへ）を保持，　backwardの最後(先頭)の出力（前のブロックへ)を保持\n",
    "        self.stateful = stateful # Falseのとき，forwardのたびに最初のRNNレイヤの隠れ状態をゼロ行列で初期化する． (ステートレス)\n",
    "    \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "    \n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape # ミニバッチ数，　文章の長さ，　分散表現の次元数(語彙数)\n",
    "        D, H = Wx.shape # 分散表現の次元数(語彙数), 隠れ層のサイズ\n",
    "        \n",
    "        self.layers = [] # forwardのたびにlayersをリセットしているが，self.hによってまた作るlayersを次のブロックとして扱える．\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "        \n",
    "        if not self.stateful or self.h is None: \n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        \n",
    "        # 長さTのRNNループを展開\n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params) # ブロック内では同じ重みを使うようだ\n",
    "            self.h = layer.forward(xs[:, t, :], self.h) # tの初回が別のブロックから渡されるかがstatefulによって決められることになる．\n",
    "            hs[:, t,  :] = self.h # 全行(ミニバッチ全て), ｔ列(ｔ番目の単語), 全チャネル(単語の分散表現ベクトル全体) の出力をこのRNNレイヤの出力とする\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0] # Wx, Wh, b\n",
    "        \n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh) # 上流からの勾配(dhs)と次の時刻(dh)の勾配を合算して下流(dxs)と前の時刻(dh)に渡す\n",
    "            dxs[:, t, :] = dx\n",
    "            \n",
    "            # Time RNNレイヤの勾配は全てのRNNレイヤの勾配の合計\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "        \n",
    "        # Time RNNレイヤのgradsに今回のbackwardの勾配をDeepコピー\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        \n",
    "        # 前の時刻のブロックの全体に渡すhの勾配\n",
    "        self.dh = dh\n",
    "    \n",
    "        return dxs # 下流への勾配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2次元の場合\n",
      "[1 2] \n",
      "\n",
      "[1 3 5] \n",
      "\n",
      "[[1]\n",
      " [3]\n",
      " [5]] \n",
      "\n",
      "3次元の場合\n",
      "[[[ 0.7244363   2.36289107  1.12872244]\n",
      "  [ 1.4658667   0.05321955 -2.10676959]]\n",
      "\n",
      " [[-0.49554816 -1.01913243 -1.60841229]\n",
      "  [-2.00337127  0.26873345 -1.24602126]]\n",
      "\n",
      " [[ 0.32095664 -0.22189829 -0.18614914]\n",
      "  [-0.71893641 -0.34189917  0.76536774]]\n",
      "\n",
      " [[ 0.65678681  0.00362619 -0.17187461]\n",
      "  [-0.03693075 -1.073377   -0.00784708]]\n",
      "\n",
      " [[-1.05595228 -0.06559255 -0.19441635]\n",
      "  [-2.72967355  0.43765403 -0.36158333]]]\n",
      "\n",
      "[[[ 0.          0.          0.        ]\n",
      "  [ 1.4658667   0.05321955 -2.10676959]]\n",
      "\n",
      " [[ 0.          0.          0.        ]\n",
      "  [-2.00337127  0.26873345 -1.24602126]]\n",
      "\n",
      " [[ 0.          0.          0.        ]\n",
      "  [-0.71893641 -0.34189917  0.76536774]]\n",
      "\n",
      " [[ 0.          0.          0.        ]\n",
      "  [-0.03693075 -1.073377   -0.00784708]]\n",
      "\n",
      " [[ 0.          0.          0.        ]\n",
      "  [-2.72967355  0.43765403 -0.36158333]]]\n",
      "\n",
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# hs[:, t, :]がよくわからんかったので実験\n",
    "\n",
    "print(\"2次元の場合\")\n",
    "A = np.array([[1,2], [3,4], [5,6]]) # コロン選択はnumpy Arrayでないとできない\n",
    "\n",
    "print(A[0, :], \"\\n\") # 0行全列\n",
    "print(A[:, 0], \"\\n\") # 全行0列 (列リストになるわけではない．1次元にされる)\n",
    "print(A[:, 0].reshape(A.shape[0], 1), \"\\n\") #　全行0列を元の形に直す\n",
    "\n",
    "print(\"3次元の場合\")\n",
    "N = 5 # ミニバッチ数\n",
    "T = 2 # 文章(コンテキスト)の長さ\n",
    "H = 3 # 隠れ層のサイズ\n",
    "\n",
    "hs = np.random.randn(N, T, H) # 5行 2列 3チャネル\n",
    "print(hs)\n",
    "\n",
    "for t in range(T):\n",
    "    print()\n",
    "    hs[:, t, :] = np.zeros((N, H), dtype='f') #　全行 ｔ列 全チャネル をゼロに， つまり文章の中の単語ごとの操作\n",
    "    print(hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 時系列データを扱うレイヤの実装\n",
    "RNNLM: RNN Language Model: RNN言語モデル: RNNによる単語の並びの自然性判定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNLMの全体図\n",
    "$w_t$ -> Embedding -> RNN -> Affine -> Softmax -> $y_t$となる．  \n",
    "RNNレイヤの出力はRNNレイヤ自身にも入力される.  \n",
    "RNNレイヤは時間方向に展開できる．  \n",
    "  \n",
    "出力層$y_t$には単語ごとの確率分布が出てくる．  \n",
    "学習済みのRNNLMにおいて，単語IDが0の入力がyouならば，その出力はsayの確率が高い分布が出力される．  \n",
    "このことから，RNNはyouの次にはsayが来る，という時系列,文脈を記憶していることがわかる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeレイヤの実装\n",
    "時間軸に展開される複数のRNNをTimeRNNとして実装したように，Affineレイヤ，EmbeddingレイヤもTime hogeレイヤとして実装する必要がある．  \n",
    "AffineレイヤもEmbeddingレイヤもただT個並べるだけだが，Affineレイヤは行列計算としてまとめて処理することで効率がよくなる．  \n",
    "TimeSoftmaxレイヤはCrossEntropyErrorレイヤと合わせて，TimeSoftmaxWithLossレイヤとして実装する．  \n",
    "TimeSoftmaxWithLossレイヤは複数のSoftmaxWithLossレイヤを持ち，それぞれがスコアと正解ラベルから，スカラで損失$L_0, L_1, ... L_{T-1}$を出力する．  \n",
    "それらの平均をとった$\\frac{1}{T} (L_0, L_1, ... L_{T-1})$がTimeSoftMaxWithLossの損失となる．  \n",
    "Timeレイヤの実装はcommon/time_layers.pyにある．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNLMの学習と評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNLMの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.time_layers import TimeEmbedding, TimeAffine, TimeSoftmaxWithLoss\n",
    "\n",
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        # 重みの初期化\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f') # Xavierの初期値， 1/sqrt(n)の標準偏差を持つ分布にする (n: 前層のノード個数)詳しくは後述\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f') # 言語モデルを扱う研究では，0.01 * np.random.uniform(...)のような初期値を扱うことが多いらしい．\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        # レイヤの初期化\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "        \n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なぜ$\\frac{1}{\\sqrt{n}}$で割ると標準偏差を$\\frac{1}{\\sqrt{n}}$にできるのか？\n",
    "$$ ave = \\frac{a + b + c}{3} $$\n",
    "$$ \\begin{eqnarray*} \n",
    "    std \n",
    "    &=&\\sqrt{ \\frac{1}{3}\\sum_{i=1}^{N} (data_i - ave)^2 } \\\\\n",
    "    &=&\\frac{1}{3\\sqrt{3}}\\sqrt{(2a-b-c)^2+(2b-a-c)^2+(2c-a-b)^2} \\\\\n",
    "    &=&\\frac{1}{3\\sqrt{3}}\\sqrt{(4a^2+b^2+c^2-4ab+2bc-4ac)+(a^2+4b^2+c^2-4ab-4bc+2ac)+(a^2+b^2+4c^2+2ab-4bc-4ac)} \\\\\n",
    "    &=&\\frac{1}{3\\sqrt{3}}\\sqrt{4a^2+a^2+a^2 +b^2+4b^2+b^2 +c^2+c^2+4c^2 -4ab-4ab+2ab +2bc-4bc-4bc -4ac+2ac-4ac} \\\\\n",
    "    &=&\\frac{1}{3\\sqrt{3}}\\sqrt{6a^2 +6b^2 +6c^2 -6ab -6bc -6ac} \\\\\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "$$ a, b, c \\leftarrow \\frac{a}{\\sqrt{3}}, \\frac{b}{\\sqrt{3}}, \\frac{c}{\\sqrt{3}} $$\n",
    "\n",
    "$$ \\begin{eqnarray*} \n",
    "    std \n",
    "    &=& \\frac{1}{3\\sqrt{3}}\\sqrt{2a^2 +2b^2 +2c^2 -2ab -2bc -2ac} \\\\\n",
    "    &=& \\frac{\\sqrt{2}}{3\\sqrt{3}}\\sqrt{a^2 +b^2 +c^2 -ab -bc -ac}\\\\\n",
    "\\end {eqnarray*} $$\n",
    "$$ \\sqrt{a^2 +b^2 +c^2 -ab -bc -ac} = \\frac{3}{\\sqrt{2}}$$\n",
    "$a,b,c$は確率変数? \n",
    "$$ \\sqrt{{V[a]}^2 +{V[b]}^2 +{V[c]}^2 -{V[a]}{V[b]} -{V[b]}{V[c]} -{V[a]}{V[c]}} = \\frac{3}{\\sqrt{2}}$$\n",
    "$V[a] = V[b] = V[c]=1$なので\n",
    "$$ \\sqrt{{V[a]}^2 +{V[b]}^2 +{V[c]}^2 -{V[a]}{V[b]} -{V[b]}{V[c]} -{V[a]}{V[c]}} = \\frac{3}{\\sqrt{2}}$$\n",
    "\n",
    "そういうわけじゃなさそう？  \n",
    "確率変数$x$の標準正規分布$f(x)$で$ x \\leftarrow \\frac{x}{\\sqrt{n}} $としたとき，標準偏差が$\\frac{\\sigma}{\\sqrt{n}}=\\frac{1}{\\sqrt{n}}$になる？\n",
    "\n",
    "標準正規分布の確率密度関数\n",
    "$$ f(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}} $$\n",
    "確率変数$x$の分散\n",
    "$$ V[x] = \\int_{-\\infty}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}} = 1$$\n",
    "．  \n",
    ".  \n",
    ".  \n",
    "わかった  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    "分散の公式\n",
    "$$ V[aX] = a^2V[X] $$\n",
    "$a = \\frac{1}{\\sqrt{n}}$とおくと\n",
    "$$ V[\\frac{1}{\\sqrt{n}}X] = \\frac{1}{n}V[X] $$\n",
    "標準偏差は\n",
    "$$ \\sqrt{\\frac{1}{n} V[X]} = \\frac{1}{\\sqrt{n}} \\sqrt{V[X]} $$\n",
    "これより，確率変数に$\\frac{1}{\\sqrt{n}}$をかけることは，標準偏差を$\\frac{1}{\\sqrt{n}}$倍することになる．  \n",
    "すなわち,標準正規分布から抽出を行うrandnで作った配列全体に$\\frac{1}{\\sqrt{n}}$をかけると標準偏差を$\\frac{1}{\\sqrt{n}} \\cdot \\sigma = \\frac{1}{\\sqrt{n}} \\cdot 1 = \\frac{1}{\\sqrt{n}}$にすることができる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語モデルの評価\n",
    "言語モデルの予測性能の良さの評価の基準として**パープレキシティ(perprexity: 困惑させるもの)**が用いられる．  \n",
    "パープレキシティは次の単語で，正解のものが出力される確率の逆数．なので低い方がいい．  \n",
    "例えば，youの次にsayが来る確率が0.8なら，パープレキシティは$\\frac{1}{0.8} = 1.25$  \n",
    "これは，次に出現する候補が1個ほどに絞れたことを意味する．  \n",
    "複数の入力データに対するパープレキシティの式は以下になる．  \n",
    "$$ L = -\\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log{y_{nk}} $$\n",
    "$$ perplexity = e^L $$\n",
    "これがどうも確率の逆数になっているらしい  \n",
    "$$ L = \\sum_{n} \\sum_{k} \\log{y_{nk} ^ {-\\frac{t_{nk}}{N}}} $$\n",
    "$$ e^L = \\exp{L} = \\exp{\\sum_{n} \\sum_{k} \\log{y_{nk} ^ {-\\frac{t_{nk}}{N}}} } = \\prod_{n} \\prod_{k} y_{nk} ^ {-\\frac{t_{nk}}{N}} =\n",
    "{ \\prod_{n} \\prod_{k} \\frac{1}{y_{nk} ^ { \\frac{t_{nk}}{N} } } }$$\n",
    "ここで，$t_{nk}$は正解の単語のみが1のone-hotベクトルなので，$k$についてプロダクトをとると，不正解の$y_{nk}^{t_{nk}}$は1になる.  \n",
    "なので，$y_n$を正解の単語の出現確率とすると，  \n",
    "$$ perplexity = \\Big( \\prod_{n} \\sqrt[N]{y_n} \\Big) ^ {-1} = \\Big( \\sqrt[N]{\\prod_{n} y_n} \\Big) ^ {-1}$$\n",
    "よってパープレキシティは，次の時刻に正解の単語が出現する確率のN乗平均の逆数をとっていることがわかる．  \n",
    "パープレキシティは情報理論の分野では，平均分岐数とも呼ばれる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNLMの学習コード\n",
    "PTBデータセットの先頭1000個のデータを使って学習を行う．  \n",
    "1000個以上になって来ると結果が悪くなる．その原因と解決方法は次章で触れる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size: 1000, vocabulary size 418\n",
      "| epoch 1 | perplexity 388.37\n",
      "| epoch 2 | perplexity 257.00\n",
      "| epoch 3 | perplexity 221.02\n",
      "| epoch 4 | perplexity 214.49\n",
      "| epoch 5 | perplexity 204.61\n",
      "| epoch 6 | perplexity 201.26\n",
      "| epoch 7 | perplexity 198.36\n",
      "| epoch 8 | perplexity 197.13\n",
      "| epoch 9 | perplexity 192.13\n",
      "| epoch 10 | perplexity 192.80\n",
      "| epoch 11 | perplexity 188.55\n",
      "| epoch 12 | perplexity 192.28\n",
      "| epoch 13 | perplexity 190.88\n",
      "| epoch 14 | perplexity 191.86\n",
      "| epoch 15 | perplexity 189.91\n",
      "| epoch 16 | perplexity 186.01\n",
      "| epoch 17 | perplexity 184.70\n",
      "| epoch 18 | perplexity 182.74\n",
      "| epoch 19 | perplexity 182.28\n",
      "| epoch 20 | perplexity 185.79\n",
      "| epoch 21 | perplexity 183.01\n",
      "| epoch 22 | perplexity 180.57\n",
      "| epoch 23 | perplexity 179.19\n",
      "| epoch 24 | perplexity 178.38\n",
      "| epoch 25 | perplexity 179.70\n",
      "| epoch 26 | perplexity 176.39\n",
      "| epoch 27 | perplexity 175.21\n",
      "| epoch 28 | perplexity 171.67\n",
      "| epoch 29 | perplexity 168.26\n",
      "| epoch 30 | perplexity 162.17\n",
      "| epoch 31 | perplexity 166.52\n",
      "| epoch 32 | perplexity 162.13\n",
      "| epoch 33 | perplexity 160.48\n",
      "| epoch 34 | perplexity 158.11\n",
      "| epoch 35 | perplexity 159.90\n",
      "| epoch 36 | perplexity 150.69\n",
      "| epoch 37 | perplexity 147.80\n",
      "| epoch 38 | perplexity 145.83\n",
      "| epoch 39 | perplexity 138.11\n",
      "| epoch 40 | perplexity 136.72\n",
      "| epoch 41 | perplexity 137.57\n",
      "| epoch 42 | perplexity 128.05\n",
      "| epoch 43 | perplexity 124.58\n",
      "| epoch 44 | perplexity 120.14\n",
      "| epoch 45 | perplexity 117.73\n",
      "| epoch 46 | perplexity 113.34\n",
      "| epoch 47 | perplexity 106.56\n",
      "| epoch 48 | perplexity 103.32\n",
      "| epoch 49 | perplexity 100.71\n",
      "| epoch 50 | perplexity 96.42\n",
      "| epoch 51 | perplexity 92.60\n",
      "| epoch 52 | perplexity 87.92\n",
      "| epoch 53 | perplexity 83.88\n",
      "| epoch 54 | perplexity 81.44\n",
      "| epoch 55 | perplexity 77.38\n",
      "| epoch 56 | perplexity 71.25\n",
      "| epoch 57 | perplexity 69.79\n",
      "| epoch 58 | perplexity 65.69\n",
      "| epoch 59 | perplexity 62.09\n",
      "| epoch 60 | perplexity 58.42\n",
      "| epoch 61 | perplexity 57.75\n",
      "| epoch 62 | perplexity 53.80\n",
      "| epoch 63 | perplexity 49.03\n",
      "| epoch 64 | perplexity 47.27\n",
      "| epoch 65 | perplexity 46.34\n",
      "| epoch 66 | perplexity 43.08\n",
      "| epoch 67 | perplexity 41.55\n",
      "| epoch 68 | perplexity 36.20\n",
      "| epoch 69 | perplexity 34.98\n",
      "| epoch 70 | perplexity 34.32\n",
      "| epoch 71 | perplexity 33.28\n",
      "| epoch 72 | perplexity 30.43\n",
      "| epoch 73 | perplexity 29.38\n",
      "| epoch 74 | perplexity 26.73\n",
      "| epoch 75 | perplexity 26.32\n",
      "| epoch 76 | perplexity 24.13\n",
      "| epoch 77 | perplexity 22.27\n",
      "| epoch 78 | perplexity 21.36\n",
      "| epoch 79 | perplexity 19.62\n",
      "| epoch 80 | perplexity 18.88\n",
      "| epoch 81 | perplexity 17.85\n",
      "| epoch 82 | perplexity 17.23\n",
      "| epoch 83 | perplexity 15.81\n",
      "| epoch 84 | perplexity 15.15\n",
      "| epoch 85 | perplexity 14.40\n",
      "| epoch 86 | perplexity 13.05\n",
      "| epoch 87 | perplexity 13.47\n",
      "| epoch 88 | perplexity 11.98\n",
      "| epoch 89 | perplexity 10.98\n",
      "| epoch 90 | perplexity 10.61\n",
      "| epoch 91 | perplexity 10.39\n",
      "| epoch 92 | perplexity 9.31\n",
      "| epoch 93 | perplexity 8.94\n",
      "| epoch 94 | perplexity 8.70\n",
      "| epoch 95 | perplexity 8.13\n",
      "| epoch 96 | perplexity 7.97\n",
      "| epoch 97 | perplexity 7.54\n",
      "| epoch 98 | perplexity 7.21\n",
      "| epoch 99 | perplexity 6.50\n",
      "| epoch 100 | perplexity 6.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from common.optimizer import SGD\n",
    "from dataset import ptb\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100 # RNNの隠れ状態ベクトルの要素数\n",
    "time_size = 5 # Truncated BPTTの展開する時間サイズ\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "# 学習データの読み込み(データセットを小さくする)\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "\n",
    "xs = corpus[:-1] # 入力\n",
    "ts = corpus[1:] # 正解ラベル\n",
    "data_size = len(xs)\n",
    "print('corpus size: %d, vocabulary size %d' % (corpus_size, vocab_size))\n",
    "\n",
    "# 学習時に使用する変数\n",
    "max_iters = data_size // (batch_size * time_size)\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "ppl_list = []\n",
    "\n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "# 1. ミニバッチの各サンプルの読み込み開始位置を計算\n",
    "jump = (corpus_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # 2. ミニバッチの取得\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size] # data範囲外になったら元に戻るようMOD演算を使用\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1 # 次の単語へ． リセットするひつようはない．\n",
    "            \n",
    "        # 勾配を求め，パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        \n",
    "    # 3. エポックごとにパープレキシティの評価\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| epoch %d | perplexity %.2f' % (epoch + 1, ppl) )\n",
    "    ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0\n",
    "    \n",
    "plt.plot(ppl_list)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('perplexity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNLMのTrainerクラス\n",
    "上記の学習を簡単に行えるように，RnnTrainerクラスが提供される．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 19 | time 0[s] | perplexity 419.36\n",
      "| epoch 2 |  iter 1 / 19 | time 0[s] | perplexity 362.18\n",
      "| epoch 3 |  iter 1 / 19 | time 0[s] | perplexity 251.74\n",
      "| epoch 4 |  iter 1 / 19 | time 0[s] | perplexity 220.23\n",
      "| epoch 5 |  iter 1 / 19 | time 0[s] | perplexity 210.60\n",
      "| epoch 6 |  iter 1 / 19 | time 0[s] | perplexity 208.56\n",
      "| epoch 7 |  iter 1 / 19 | time 0[s] | perplexity 201.84\n",
      "| epoch 8 |  iter 1 / 19 | time 0[s] | perplexity 200.75\n",
      "| epoch 9 |  iter 1 / 19 | time 0[s] | perplexity 194.55\n",
      "| epoch 10 |  iter 1 / 19 | time 0[s] | perplexity 189.76\n",
      "| epoch 11 |  iter 1 / 19 | time 0[s] | perplexity 192.67\n",
      "| epoch 12 |  iter 1 / 19 | time 0[s] | perplexity 189.60\n",
      "| epoch 13 |  iter 1 / 19 | time 0[s] | perplexity 192.54\n",
      "| epoch 14 |  iter 1 / 19 | time 0[s] | perplexity 186.54\n",
      "| epoch 15 |  iter 1 / 19 | time 0[s] | perplexity 186.34\n",
      "| epoch 16 |  iter 1 / 19 | time 0[s] | perplexity 191.00\n",
      "| epoch 17 |  iter 1 / 19 | time 0[s] | perplexity 189.69\n",
      "| epoch 18 |  iter 1 / 19 | time 0[s] | perplexity 184.19\n",
      "| epoch 19 |  iter 1 / 19 | time 0[s] | perplexity 180.67\n",
      "| epoch 20 |  iter 1 / 19 | time 0[s] | perplexity 181.13\n",
      "| epoch 21 |  iter 1 / 19 | time 0[s] | perplexity 178.48\n",
      "| epoch 22 |  iter 1 / 19 | time 1[s] | perplexity 179.35\n",
      "| epoch 23 |  iter 1 / 19 | time 1[s] | perplexity 180.36\n",
      "| epoch 24 |  iter 1 / 19 | time 1[s] | perplexity 177.39\n",
      "| epoch 25 |  iter 1 / 19 | time 1[s] | perplexity 170.49\n",
      "| epoch 26 |  iter 1 / 19 | time 1[s] | perplexity 172.92\n",
      "| epoch 27 |  iter 1 / 19 | time 1[s] | perplexity 172.04\n",
      "| epoch 28 |  iter 1 / 19 | time 1[s] | perplexity 172.36\n",
      "| epoch 29 |  iter 1 / 19 | time 1[s] | perplexity 166.18\n",
      "| epoch 30 |  iter 1 / 19 | time 1[s] | perplexity 161.17\n",
      "| epoch 31 |  iter 1 / 19 | time 1[s] | perplexity 158.25\n",
      "| epoch 32 |  iter 1 / 19 | time 1[s] | perplexity 154.48\n",
      "| epoch 33 |  iter 1 / 19 | time 1[s] | perplexity 152.27\n",
      "| epoch 34 |  iter 1 / 19 | time 1[s] | perplexity 152.59\n",
      "| epoch 35 |  iter 1 / 19 | time 1[s] | perplexity 144.65\n",
      "| epoch 36 |  iter 1 / 19 | time 1[s] | perplexity 142.67\n",
      "| epoch 37 |  iter 1 / 19 | time 1[s] | perplexity 141.99\n",
      "| epoch 38 |  iter 1 / 19 | time 1[s] | perplexity 131.48\n",
      "| epoch 39 |  iter 1 / 19 | time 1[s] | perplexity 128.46\n",
      "| epoch 40 |  iter 1 / 19 | time 1[s] | perplexity 123.72\n",
      "| epoch 41 |  iter 1 / 19 | time 2[s] | perplexity 117.99\n",
      "| epoch 42 |  iter 1 / 19 | time 2[s] | perplexity 116.86\n",
      "| epoch 43 |  iter 1 / 19 | time 2[s] | perplexity 112.24\n",
      "| epoch 44 |  iter 1 / 19 | time 2[s] | perplexity 108.67\n",
      "| epoch 45 |  iter 1 / 19 | time 2[s] | perplexity 100.07\n",
      "| epoch 46 |  iter 1 / 19 | time 2[s] | perplexity 98.46\n",
      "| epoch 47 |  iter 1 / 19 | time 2[s] | perplexity 97.52\n",
      "| epoch 48 |  iter 1 / 19 | time 2[s] | perplexity 92.17\n",
      "| epoch 49 |  iter 1 / 19 | time 2[s] | perplexity 86.99\n",
      "| epoch 50 |  iter 1 / 19 | time 2[s] | perplexity 82.78\n",
      "| epoch 51 |  iter 1 / 19 | time 2[s] | perplexity 80.82\n",
      "| epoch 52 |  iter 1 / 19 | time 2[s] | perplexity 75.28\n",
      "| epoch 53 |  iter 1 / 19 | time 2[s] | perplexity 71.25\n",
      "| epoch 54 |  iter 1 / 19 | time 2[s] | perplexity 68.47\n",
      "| epoch 55 |  iter 1 / 19 | time 2[s] | perplexity 65.26\n",
      "| epoch 56 |  iter 1 / 19 | time 2[s] | perplexity 61.53\n",
      "| epoch 57 |  iter 1 / 19 | time 2[s] | perplexity 61.18\n",
      "| epoch 58 |  iter 1 / 19 | time 2[s] | perplexity 54.79\n",
      "| epoch 59 |  iter 1 / 19 | time 2[s] | perplexity 54.38\n",
      "| epoch 60 |  iter 1 / 19 | time 3[s] | perplexity 50.91\n",
      "| epoch 61 |  iter 1 / 19 | time 3[s] | perplexity 46.38\n",
      "| epoch 62 |  iter 1 / 19 | time 3[s] | perplexity 44.67\n",
      "| epoch 63 |  iter 1 / 19 | time 3[s] | perplexity 42.97\n",
      "| epoch 64 |  iter 1 / 19 | time 3[s] | perplexity 39.89\n",
      "| epoch 65 |  iter 1 / 19 | time 3[s] | perplexity 36.84\n",
      "| epoch 66 |  iter 1 / 19 | time 3[s] | perplexity 36.14\n",
      "| epoch 67 |  iter 1 / 19 | time 3[s] | perplexity 33.48\n",
      "| epoch 68 |  iter 1 / 19 | time 3[s] | perplexity 32.37\n",
      "| epoch 69 |  iter 1 / 19 | time 3[s] | perplexity 30.29\n",
      "| epoch 70 |  iter 1 / 19 | time 3[s] | perplexity 28.67\n",
      "| epoch 71 |  iter 1 / 19 | time 3[s] | perplexity 27.90\n",
      "| epoch 72 |  iter 1 / 19 | time 3[s] | perplexity 25.89\n",
      "| epoch 73 |  iter 1 / 19 | time 3[s] | perplexity 23.61\n",
      "| epoch 74 |  iter 1 / 19 | time 3[s] | perplexity 23.44\n",
      "| epoch 75 |  iter 1 / 19 | time 3[s] | perplexity 22.63\n",
      "| epoch 76 |  iter 1 / 19 | time 3[s] | perplexity 22.56\n",
      "| epoch 77 |  iter 1 / 19 | time 3[s] | perplexity 19.22\n",
      "| epoch 78 |  iter 1 / 19 | time 4[s] | perplexity 18.08\n",
      "| epoch 79 |  iter 1 / 19 | time 4[s] | perplexity 17.08\n",
      "| epoch 80 |  iter 1 / 19 | time 4[s] | perplexity 17.10\n",
      "| epoch 81 |  iter 1 / 19 | time 4[s] | perplexity 16.16\n",
      "| epoch 82 |  iter 1 / 19 | time 4[s] | perplexity 15.09\n",
      "| epoch 83 |  iter 1 / 19 | time 4[s] | perplexity 14.29\n",
      "| epoch 84 |  iter 1 / 19 | time 4[s] | perplexity 13.45\n",
      "| epoch 85 |  iter 1 / 19 | time 4[s] | perplexity 12.54\n",
      "| epoch 86 |  iter 1 / 19 | time 4[s] | perplexity 11.78\n",
      "| epoch 87 |  iter 1 / 19 | time 4[s] | perplexity 11.62\n",
      "| epoch 88 |  iter 1 / 19 | time 4[s] | perplexity 10.43\n",
      "| epoch 89 |  iter 1 / 19 | time 4[s] | perplexity 10.23\n",
      "| epoch 90 |  iter 1 / 19 | time 4[s] | perplexity 10.13\n",
      "| epoch 91 |  iter 1 / 19 | time 4[s] | perplexity 9.84\n",
      "| epoch 92 |  iter 1 / 19 | time 4[s] | perplexity 9.58\n",
      "| epoch 93 |  iter 1 / 19 | time 4[s] | perplexity 8.60\n",
      "| epoch 94 |  iter 1 / 19 | time 4[s] | perplexity 8.26\n",
      "| epoch 95 |  iter 1 / 19 | time 4[s] | perplexity 8.19\n",
      "| epoch 96 |  iter 1 / 19 | time 4[s] | perplexity 7.75\n",
      "| epoch 97 |  iter 1 / 19 | time 4[s] | perplexity 7.30\n",
      "| epoch 98 |  iter 1 / 19 | time 4[s] | perplexity 6.71\n",
      "| epoch 99 |  iter 1 / 19 | time 4[s] | perplexity 6.37\n",
      "| epoch 100 |  iter 1 / 19 | time 5[s] | perplexity 6.03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from common.trainer import RnnlmTrainer\n",
    "\n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size)\n",
    "\n",
    "plt.plot(trainer.ppl_list)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('perplexity')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
