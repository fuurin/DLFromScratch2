{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで我々が見てきたNNは，フィードフォワードと呼ばれるネットワークで，流れが1方向でしかなかった.  \n",
    "フィードフォワードでは時系列データをうまく扱うことができない．  \n",
    "そこでリカレントニューラルネットワーク(RNN)の出番である．  \n",
    "本章では，フィードフォワードの問題点を指摘し，RNNがその問題を解決することを示す．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確率と言語モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vecを確率の視点から眺める\n",
    "\n",
    "コーパス$w_1, w_2, ... w_T$が与えられているとき，$w_t$がターゲットとなる確率は，コンテキスト$w_{t-1}, w{t+1}$を使って\n",
    "$$ P(w_t | w_{t-1}, w_{t+1})$$\n",
    "と書ける．  \n",
    "ここで，コンテキストの窓を非対称にして全て左側にコンテキストがあるとすると，$w_t$がターゲットとなる確率は\n",
    "$$ P(w_t | w_{t-2}, w_{t-1})$$\n",
    "このとき，CBOWモデルが扱う損失関数は\n",
    "$$ L=-\\log P(w_t|w_{t-2}, w_{t-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 言語モデル\n",
    "言語モデルは，単語の並びがどれだけ自然であるかを確率で評価する．  \n",
    "例えば，\n",
    "- you say goodbye -> 0.092\n",
    "- you say good die -> 0.000000000000032  \n",
    "\n",
    "$ w_1, ..., w_m $ という順序で単語が出現する確率は，同時確率 $ P(w_1, ..., w_m) $ で表される．  \n",
    "これを事後確率と確率の乗法定理 $P(A, B) = P(A|B)P(B)$ を使って分解すると　　\n",
    "$$ P(w_1, ..., w_m) = P(w_m|w_1, ... w_{m-1})P(w_{m-1}|w_1, ... w_{m-2}) ... P(w_3|w_1, w_2)P(w_2|w_1)P(w_1)$$  \n",
    "$$ = \\prod_{t=1}^{m} P(w_t|w_1, ..., w_{t-1})$$\n",
    "\n",
    "確率の乗法定理は，「AとBが両方同時に起こる確率 $P(A,B)$」は，「Bが起こる確率$P(B)$」と「Bが起こったあとにAが起きる確率P(A|B)」を掛け合わせたものである． \n",
    "また，$P(A, B) = P(B|A)P(A)$と書くこともできる．  \n",
    "ここで注目すべきは，事後確率が対象の単語より左の全ての単語をコンテキストとした時の確率ということである．  \n",
    "また， $P(w_t|w_1, ... w_{t-1})$を表すモデルは，条件付き言語モデルと呼ばれる．これを言語モデルと呼ぶ場合も多く見られる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOWモデルを言語モデルに？\n",
    "CBOWモデルを無理やり言語モデルに適用するには，コンテキストのサイズをある値に限定することで近似的に表すことができる．  \n",
    "$$ P(w_1, ..., w_m) = \\prod_{t=1}^{m} P(w_t|w_1, ..., w_{t-1}) \\approx \\prod_{t=1}^{m} P(w_t|w_{t-2}, w_{t-1}) $$\n",
    "  \n",
    "マルコフ性  \n",
    "未来の状態が現在の状態だけに依存して決まること．  \n",
    "ここで，ある事象の確率がその直前のN個の事象だけに依存するとき，これを「N階マルコフ連鎖」という． 　\n",
    "今は直前の2つに依存して次の単語が決まるので，2階マルコフ連鎖と呼べる．  \n",
    "  \n",
    "コンテキストのサイズは任意に設定できるが，固定する必要があるところに問題がある．  \n",
    "例えば，コンテキストのサイズが10だが答えとなるTomなどの固有名詞が18個前にしかない時，この推論問題に答えることはできない．  \n",
    "コンテキストを20にしたりすれば答えることはできる．  \n",
    "  \n",
    "しかし次にはコンテキスト内の単語の並びが無視されるという問題がある．  \n",
    "例えば，(you,say)というコンテキストと(say,you)というコンテキストが同じものとして表される．  \n",
    "これは，CBOWモデルの中間層を各コンテキストが共有しているために起きる．  \n",
    "そこで，コンテキストごとに中間層を設けることで，すなわち複数の中間層を「連結(contcatenate)」することでこの問題に対処できる．  \n",
    "しかし，そのようにするとコンテキストのサイズに比例して重みパラメータが増大してしまう．  \n",
    "\n",
    "これらの問題を解決するのがRNNである．  \n",
    "RNNは，コンテキストがどれだけ長くても，そのコンテキストの情報を記憶するメカニズムを持つ．  \n",
    "  \n",
    "ちなみに，実はword2vecの方が後に提案されている．  \n",
    "RNNによる言語モデルでも単語の分散表現を獲得できるのだが，新たな語彙の追加しやすさや単語の分散表現の質の向上のためword2vecが提案された．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNとは\n",
    "Recurrent Neural Networkは日本語では「再起ニューラルネットワーク」や「循環ニューラルネットワーク」と呼ばれる．  \n",
    "これに対してRecursive Neural Networkというものもあるが，こちらは主に木構造のデータを処理するためのネットワークで，RNNとは別物である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循環するニューラルネットワーク\n",
    "RNNの特徴は，閉路を持つことである．  \n",
    "入力データを$(x_0, x_1, ... x_t, ...)$として， 出力データ$(h_0, h_1, ... h_t, ...)$ を自分にも入力する，すなわち閉路を持つ層をRNNレイヤと名付ける．　　\n",
    "ここで，　$x_t$や$h_t$はベクトルを想定する．例えば，ある単語の分散表現を$x_t$としたりする．  \n",
    "また，これまではデータが左から右へ流れていたが，以降は左右方向にレイヤが展開されるため，紙面の都合上，下から上へデータが流れるように描画される．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ループの展開\n",
    "RNNレイヤは自身に出力データを流していたが，これを同じレイヤの別のRNNレイヤに流すことで，ループを展開する．  \n",
    "左から右へ，同じレイヤのRNNレイヤが並び，その順が時系列の順番になっている．  \n",
    "その出力が，左から$h_0, h_1, ... h_t$となる．  \n",
    "「t番目の単語」や「t番目のRNNレイヤ」は，「時刻tの単語」や「時刻tのRNNレイヤ」というように，「時刻」という言葉でも表される．  \n",
    "このとき，$h_t$は以下の計算で算出される．  \n",
    "$$ h_t = \\tanh (h_{t-1} W_h + x_t W_x + b) $$\n",
    "RNNレイヤは重みを2つ持つ．  \n",
    "入力$x$への重み$W_x$と一つ前のRNNの出力$h_{t-1}$に対する重み$W_h$である．  \n",
    "これらの和にさらにバイアス$b$を加え， 活性化関数としてtanhを適用したものがRNNレイヤの出力になっている．  \n",
    "  \n",
    "この式では，RNNが$h_t$という現在の出力を状態として保持していて，上記の式で$h_t$が更新されていると見ることもできる．  \n",
    "そのため，RNNレイヤは状態を持つレイヤやメモリを持つレイヤと呼ばれている．  \n",
    "  \n",
    "この$h_t$は「隠れ状態」や「隠れ状態ベクトル」と呼ばれている．  \n",
    "  \n",
    "多くの文献では，RNNレイヤの次の時刻への矢印はRNNレイヤから生えているが，本書では同じデータがコピーして分岐されたものであることを明示的に示すため，RNNレイヤの次の層への矢印から次の自国への矢印を伸ばすことにする．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time\n",
    "ループを展開した後のRNNは，時間方向と逆方向に誤差逆伝播法を使って重みを更新していくことができる．  \n",
    "このようなRNNにおける誤差逆伝播ん法をBackpropagation Through Time, 略してBPTTと呼ぶ.  \n",
    "  \n",
    "しかし，RNNレイヤを長く繋げると計算リソース(RNNが使うメモリ)が膨大になり，逆伝播の勾配も不安定になってしまう．　　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated BPTT\n",
    "時間方向に長くなりすぎたネットワークを適当な場所で切り取って(truncate)小さなネットワークを複数作ること．  \n",
    "これら小さくなったネットワークに対して，誤差逆伝播法を行う．  \n",
    "正確には，ネットワークの逆伝播の時だけつながりを断ち切る．順伝播はそのまま行う．  \n",
    "  \n",
    "具体例を見ていく.  \n",
    "1000個の長さのコーパス$x_0, x_1, ... x_{999}$があるとする． このコーパスは，複数の文が連結されたものであるとする．  \n",
    "Truncated BPTTをするため，逆伝播のときは1000個連結されたRNNを「ブロック」に切り分ける．  \n",
    "ここでは，それぞれのブロックのRNNのレイヤの長さが10になるようにする．  これによって，それぞれのブロックではそれより未来のデータについて考えなくていい．  \n",
    "\n",
    "順伝播の時は，データを順番に(シーケンシャルに)与える必要がある．  \n",
    "これまでミニバッチ学習ではデータをランダムに選んでいたが，Truncated BPTTでは順伝播の時にデータをシーケンシャルに与える必要がある．  \n",
    "  \n",
    "上の例でいうと，まずは一つ目のブロックの入力データ$x_0, ... x_9$をRNNレイヤに与え, forwardとbackwardを行う．  \n",
    "次は，入力データ$x_{10}, ... ,x_{19}$をRNNに与え, forwardとbackwardを行う．  \n",
    "このとき，前のブロックの出力$h_9$をこのブロックの先頭のRNNに与える必要があることに注意する．これによって順伝播のつながりは維持できる.  \n",
    "以上のようにして順伝播とTruncated BPTTを進めていく．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated BPTTのミニバッチ学習\n",
    "RNNでTruncated BPTTする場合のミニバッチ学習では，開始位置をずらしたデータを利用する．  \n",
    "これによって，各単語の前後関係を保ったまま，つまりシーケンシャルなまま，違うデータをRNNに与えることができる．  \n",
    "\n",
    "例えば上の例では$x_0$からデータを与えていったが，別のデータでは$x_500$からデータを与えていく．  \n",
    "なお，途中で終端に達した場合は，先頭に戻る．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNの実装\n",
    "横方向にRNNをT個だけ繋げたものを一つの層として実装する．  \n",
    "この層は，シーケンシャルなデータ$xs = (x_0, x_1, ... x_{T-1})$を入力すると，隠れ層へ$hs=(h_0, h_1, ..., h_{T-1})$を出力する．  \n",
    "このひとまとまりにした層を「Time RNN」レイヤと呼ぶ．↔️ 1ステップの処理を行う「RNNレイヤ」  \n",
    "後ほどTime Affine レイヤやTime Embeddingレイヤといった，時系列データをまとめて処理するレイヤも登場する．  \n",
    "\n",
    "まず1ステップの処理を行うRNNクラスを作成し，Tステップの処理をまとめて行うレイヤをTimeRNNクラスとして完成させる．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNレイヤの実装\n",
    "RNNレイヤの計算式を再掲\n",
    "$$ h_t = \\tanh (h_{t-1} W_h + x_t W_x + b) $$\n",
    "$x_t$と$h_{t-1}$はミニバッチ処理のため，$N$個のデータを持つとする．各行がデータで，それらが列方向に並ぶ．  \n",
    "そのため，$x_t$と$h_{t-1}$はそれぞれ$N \\times H $, $N \\times D$の行列になる．  \n",
    "ここで，$H$は隠れ状態ベクトルの次元数， $D$は入力データの次元数である．  \n",
    "RNNレイヤの計算では，次元数が以下のようになることに注意するのが重要である．\n",
    "\n",
    "$$ h_{t-1} W_h + x_t W_x + b \\Rightarrow h_t$$\n",
    "$$ (N \\times H) (H\\times H) + (N \\times D) (D \\times H) + (N \\times H) \\Rightarrow (N \\times H) $$  \n",
    "\n",
    "以下この式とシステムに従いRNNレイヤを実装する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b] # 初期重みをparamsに設定\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)] # 初期勾配をgradsに設定\n",
    "        self.cache = None # BPTT用のキャッシュ変数(メモリ)\n",
    "    \n",
    "    def forward(self, x, h_prev): # 入力データと前の時刻の出力\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "        \n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "    \n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        # tanhの逆伝播\n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        \n",
    "        # 加算ノードはdtをそのまま前のレイヤに返す\n",
    "        \n",
    "        # b方向へのrepeatの逆伝播\n",
    "        db = np.sum(dt, axis=0)\n",
    "        \n",
    "        # h_prev方向へのMatMulの逆伝播\n",
    "        dWh = h_prev.T @ dt\n",
    "        dh_prev = dt @ Wh.T\n",
    "        \n",
    "        # x方向へのMatMulの逆伝播\n",
    "        dWx = x.T @ dt\n",
    "        dx = dt @ Wx.T\n",
    "        \n",
    "        self.grads[0][...] = dWx # 3点リーダによって，浅いコピー(参照のコピー)ではなく深いコピー(値のコピー)を行う．\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでちょっと逆伝播の復習\n",
    "\n",
    "- 加算ノードの逆伝播  \n",
    "上流から入ってきた勾配$\\frac{\\partial L}{\\partial z}$をそのまま受け流す. 3個以上の加算ノードでも同様にそのまま受け流す．\n",
    "$$ z = x + y $$\n",
    "$$ \\frac{\\partial z}{\\partial x} = 1, \\frac{\\partial z}{\\partial y} = 1 $$\n",
    "上流で発生した損失Lに対するxの勾配\n",
    "$$　\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial x} = \\frac{\\partial L}{\\partial z} \\cdot 1 = \\frac{\\partial L}{\\partial z}$$\n",
    "上流で発生した損失Lに対するyの勾配\n",
    "$$ \\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial y} = \\frac{\\partial L}{\\partial z} \\cdot 1 = \\frac{\\partial L}{\\partial z}$$  \n",
    "\n",
    "<br><br>\n",
    "- Repeatノードの逆伝播\n",
    "RNNレイヤの実装では，N個のミニバッチにそれぞれ同じ重さベクトルbを加算している．  \n",
    "これは分岐ノードの一般形であるRepeatノードの処理が行われていることになる．  \n",
    "分岐ノード，Repeatノードでは，上流からやってきた重みを全て足し合わせて下流に返す．  \n",
    "Repeatノードの逆伝播は，$D\\times N$の上流からの勾配をN個の方向に足し合わせ，D次元のベクトルを作り出し，そのベクトルを勾配とする．\n",
    "\n",
    "<br><br>\n",
    "- 行列の積(Matrix Multiply: MatMul)の逆伝播  \n",
    "p30, 31ではシグマを使ってもうちょい厳密に書いているが，自分の理解を書いてみる．まずは結果から\n",
    "$$ y = xW $$\n",
    "$$ (N \\times H) = (N \\times D)(D \\times H) $$\n",
    "$$ \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y}W^{\\top}, \\frac{\\partial L}{\\partial W} = x^{\\top} \\frac{\\partial L}{\\partial y} $$\n",
    "$$ (N \\times D) = (N \\times H)(H \\times D), (D \\times H) = (D \\times N)(N \\times H) $$\n",
    "<br>\n",
    "\n",
    "例えば，以下のような場合を考える．\n",
    "$$ y = xW = \\left( \\begin {array}{ccc} a & b & c \\end{array} \\right) \\left( \\begin {array}{ccc} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{array} \\right) = \\left( \\begin {array}{ccc} 1a+2b+3c & 4a+5b+6c & 7a+8b+9c \\end {array} \\right) $$\n",
    "\n",
    "このとき， [ベクトルをベクトルで微分する計算の定義](https://qiita.com/AnchorBlues/items/8fe2483a3a72676eb96d)を使って\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\left( \\begin{array}{ccc} \\frac{\\partial y_1}{\\partial a} & \\frac{\\partial y_2}{\\partial a} & \\frac{\\partial y_3}{\\partial a}\\\\ \\frac{\\partial y_1}{\\partial b} & \\frac{\\partial y_2}{\\partial b} & \\frac{\\partial y_3}{\\partial b} \\\\ \\frac{\\partial y_3}{\\partial c} & \\frac{\\partial y_2}{\\partial c} & \\frac{\\partial y_3}{\\partial c}\\end{array} \\right) =\\left( \\begin{array}{ccc} 1 & 4 & 7 \\\\ 2 & 5 & 8 \\\\ 3 & 6 & 9 \\end{array} \\right) = W^\\top$$\n",
    "つまり，\n",
    "$$ \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} = \\frac{\\partial L}{\\partial y} W^\\top $$ \n",
    "$\\frac{\\partial L}{\\partial W}　= x^\\top \\frac{\\partial L}{\\partial y}$も同様に求められる．(ベクトルを行列で微分)\n",
    "<br><br>\n",
    "- tanhの逆伝播\n",
    "$$ y = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "の微分を求める．商の微分法を使って\n",
    "$$ \\frac{\\partial \\tanh(x)}{\\partial x} = \\frac {({e^x + e^{-x}})({e^x + e^{-x}}) - ({e^x - e^{-x}})({e^x - e^{-x}})}{({e^x + e^{-x}})^2} $$\n",
    "$$ = 1 - \\Bigl\\{ \\frac {({e^x - e^{-x}})}{({e^x + e^{-x}})} \\Bigr\\}^2 $$\n",
    "$$ = 1 - \\tanh(x)^2 = 1 - y^2 $$\n",
    "よって, 上流からの勾配$\\frac{\\partial L} {\\partial y}$を使って\n",
    "$$ \\frac{\\partial L} {\\partial x} = \\frac{\\partial L} {\\partial y} \\cdot \\frac{\\partial y} {\\partial x} = \\frac{\\partial L} {\\partial y} (1 - y^2) $$\n",
    "を下流に返せば良い．  \n",
    "ここでyは順伝播の時に保持しておく．RNNレイヤの実装でいうh_next， 最終的な出力を使うことになる．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time RNN レイヤの実装\n",
    "T個のRNNレイヤで構成されるTime RNNレイヤを実装する．  \n",
    "Time RNNレイヤは隠れ状態hをメンバ変数として保持し，ブロックに分割した時に次のブロックへhを渡す．  \n",
    "隠れ状態を引き継ぐかどうかはstatefulという引数で調整する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False): # stateful: 状態を持つ，という意味\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None # 複数のRNNレイヤをリストとして持つ予定\n",
    "        self.h, self.dh = None, None # forwardの最後の出力(次のブロックへ）を保持，　backwardの最後(先頭)の出力（前のブロックへ)を保持\n",
    "        self.stateful = stateful # Falseのとき，forwardのたびに最初のRNNレイヤの隠れ状態をゼロ行列で初期化する． (ステートレス)\n",
    "    \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.h = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
